{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noita 'Rickroll' QR Code Noise Analysis From Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/probable-basilisk/noita_qr_analysis/blob/main/qr_noise_analysis.ipynb)\n",
    "\n",
    "\n",
    "I have heard it said that nobody has been able to 'replicate' this analysis so here it is, in a conveniently\n",
    "runnable colab notebook, totally from scratch starting with nothing more than the qrcode image from in game\n",
    "and ending with both the binary and grayscale noise tables and the list of row offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running in colab, you'll need to run this cell to download the qrcode and moby dick\n",
    "!wget -O qr.png https://github.com/probable-basilisk/noita_qr_analysis/blob/main/qr.png?raw=true\n",
    "!wget -O mobydick.png https://github.com/probable-basilisk/noita_qr_analysis/blob/main/mobydick.png?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import operator\n",
    "import collections\n",
    "import json\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data Prep / Isolating the Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how to write this image as a png in a sensible way\n",
    "def omniwrite(fn, img):\n",
    "  img = np.copy(img.astype(np.uint8))\n",
    "  if np.max(img) == 1:\n",
    "    img *= 255\n",
    "  if len(img.shape) < 3 or img.shape[2] == 1:\n",
    "    img = np.dstack([img]*3)\n",
    "  cv2.imwrite(fn, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrcode = cv2.imread(\"qr.png\")\n",
    "plt.imshow(qrcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First observations: the qrcode blocks are perfectly aligned to a 12x12 pixel grid\n",
    "# This will simplify a lot of stuff\n",
    "BLOCKSIZE = 12\n",
    "IMAGEHEIGHT = qrcode.shape[0]\n",
    "ROWSIZE = qrcode.shape[1]\n",
    "\n",
    "def stack(img):\n",
    "  return np.vstack([img[:,:,c] for c in range(3)])\n",
    "\n",
    "def destack(stacked):\n",
    "  return np.dstack([stacked[i*IMAGEHEIGHT:(i+1)*IMAGEHEIGHT] for i in range(3)])\n",
    "\n",
    "def blockpos(row, col):\n",
    "  return (row*BLOCKSIZE, col*BLOCKSIZE)\n",
    "\n",
    "def extract_block(img, row, col, nrows=1, ncols=1):\n",
    "  return img[row*BLOCKSIZE:(row+nrows)*BLOCKSIZE, col*BLOCKSIZE:(col+ncols)*BLOCKSIZE, :]\n",
    "\n",
    "# Let's have a closer look at a white and black block\n",
    "_ = plt.imshow(extract_block(qrcode, 5, 4, 1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at some histograms\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Grayscale Histograms For Entire Image, White and Black Blocks')\n",
    "\n",
    "ax1.set_title(\"Entire Image\")\n",
    "_ = ax1.hist(qrcode.reshape((-1)), bins=32)\n",
    "\n",
    "ax2.set_title(\"White Block\")\n",
    "_ = ax2.hist(extract_block(qrcode, 5, 4).reshape((-1)), bins=32)\n",
    "\n",
    "ax3.set_title(\"Black Block\")\n",
    "_ = ax3.hist(extract_block(qrcode, 5, 5).reshape((-1)), bins=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "* a white block has a large number of 255s\n",
    "* a black block a large number of 0s\n",
    "* the histograms of white and black blocks are mirrors of each other.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "We can figure out if a block is 'white' or 'black' by just checking if it has\n",
    "more 0s or 255s, and then we can turn white blocks into black blocks by\n",
    "inverting the RGB values as `new_value = 255 - old_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_block(block):\n",
    "  is_white = (np.sum(block == 255) > np.sum(block == 0))\n",
    "  if is_white:\n",
    "    return (1, 255 - block)\n",
    "    #return (1, block ^ 0xFF)\n",
    "  else:\n",
    "    return (0, block)\n",
    "\n",
    "def isolate_noise_from_qrcode(img):\n",
    "  noise_only = np.zeros_like(img)\n",
    "  qrcode_only = np.zeros(img.shape[:2]).astype(int)\n",
    "  nrows = img.shape[0] // BLOCKSIZE\n",
    "  ncols = img.shape[1] // BLOCKSIZE\n",
    "  for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "      y, x = blockpos(row, col)\n",
    "      (color, noise) = clean_block(extract_block(img, row, col))\n",
    "      qrcode_only[y:y+BLOCKSIZE, x:x+BLOCKSIZE] = color\n",
    "      noise_only[y:y+BLOCKSIZE, x:x+BLOCKSIZE, :] = noise\n",
    "  return (qrcode_only, noise_only)\n",
    "\n",
    "qrcode_clean, isolated_noise = isolate_noise_from_qrcode(qrcode)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Separated QRCode and Noise')\n",
    "ax1.set_title('Clean QR Code')\n",
    "ax1.imshow(qrcode_clean)\n",
    "ax2.set_title('Isolated Noise')\n",
    "ax2.imshow(isolated_noise)\n",
    "ax3.set_title('Histogram of Isolated Noise')\n",
    "_ = ax3.hist(isolated_noise.reshape((-1)), bins=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have now produced noise that has the histogram (distribution) of values in black blocks.\n",
    "\n",
    "Our final step of data preparation is to split the noise into three images, one for each RGB channel, and produce a *binary*\n",
    "version of the noise. As part of this prep, we also *XOR* the binary noise again with the clean QR code. \n",
    "\n",
    "Why? It makes the analysis work. \n",
    "\n",
    "The fact that you have to separately XOR the QRCode back into the binary noise is the reason\n",
    "why you could reasonably say that there's a second level of rickroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_binary_noise(noise, clean_qr_code, double_xor):\n",
    "  if double_xor:\n",
    "    return np.logical_xor(noise > 0, clean_qr_code > 0)\n",
    "  else:\n",
    "    return noise > 0\n",
    "\n",
    "def prep_noise(isolated_noise, clean_qr_code, double_xor):\n",
    "  grayscale_noise = [isolated_noise[:, :, i] for i in range(3)]\n",
    "  binary_noise = [prep_binary_noise(n, clean_qr_code, double_xor) for n in grayscale_noise]\n",
    "  return (grayscale_noise, binary_noise)\n",
    "\n",
    "def full_preprocess(src_image, double_xor):\n",
    "  clean, noise = isolate_noise_from_qrcode(src_image)\n",
    "  return prep_noise(noise, clean, double_xor)\n",
    "\n",
    "(grayscale_noise, binary_noise) = prep_noise(isolated_noise, qrcode_clean, double_xor=True)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('Prepped Noise (Green Channel)')\n",
    "ax1.set_title('Grayscale Noise (zoomed)')\n",
    "ax1.imshow(grayscale_noise[1][0:100, 0:100])\n",
    "ax2.set_title('Binary Noise (zoomed)')\n",
    "_ = ax2.imshow(binary_noise[1][0:100, 0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Extracting the Noise Table & Row Offsets\n",
    "\n",
    "All the binary noise is actually drawn from a relatively small table of values: each row is simply a slice of a 4559 large noise table.\n",
    "Here we extract that table *from scratch* starting with nothing more than the assumption that every row of binary noise\n",
    "comes out of some larger noise sequence.\n",
    "\n",
    "First, we're going to look for rows that have *exact overlaps* with other rows. To do this efficiently (i.e., without O(n^2) comparing\n",
    "every row to every other row) we will build a hash table of every `MATCH_BITS` window of every row, and then check each row's\n",
    "*tail* (the last `MATCH_BITS` of that row) against the hash table. Here I set `MATCH_BITS` to 32 so that the probability of accidentally\n",
    "matching rows is so incredibly low that we can ignore the possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, dealing with three color channels separately is annoying, so\n",
    "# we're going to just stack the color channels into one 3x tall image\n",
    "binary_noise_stacked = np.vstack(binary_noise)\n",
    "magnitude_noise_stacked = np.vstack([isolated_noise[:,:,c] for c in range(3)])\n",
    "\n",
    "# How many bits of exact match we need to link two rows.\n",
    "# Note that 32 bits of exact match means the likelihood of\n",
    "# a false positive, assuming uniform random binary noise, is about\n",
    "# one in four billion. In other words, we would have to get\n",
    "# extraordinarily unlucky to accidentally match two rows by chance.\n",
    "MATCH_BITS = 32\n",
    "\n",
    "# Convenience for easily turning a binary array into an integer\n",
    "PLACE_VALUES = 2 ** np.arange(MATCH_BITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build up the hash table and then prune it to produce a directed graph of overlapping rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the last MATCH_BITS values of each row into an integer\n",
    "# (just treat it as a binary number)\n",
    "def calculate_row_tails(img):\n",
    "  img = img.astype(int)\n",
    "  return img[:, -MATCH_BITS:] @ PLACE_VALUES\n",
    "\n",
    "# Create a dictionary that maps a *tail* (integer value)\n",
    "# to the possible next rows that link to it\n",
    "def create_possible_successors_map(noise):\n",
    "  noise = noise.astype(int)\n",
    "  successors = collections.defaultdict(set)\n",
    "  # We iterate to width-1 because we do not want rows\n",
    "  # to match with themselves: a successor needs to actually\n",
    "  # *grow* the chain.\n",
    "  for offset in range(noise.shape[1]-MATCH_BITS-1):\n",
    "    windows = noise[:, offset:offset+MATCH_BITS]\n",
    "    hashes = windows @ PLACE_VALUES\n",
    "    for row, val in enumerate(hashes):\n",
    "      successors[val].add((row, offset))\n",
    "  return successors\n",
    "\n",
    "# Count how many bits match between the tail of row_a\n",
    "# and the candidate position pos in row_b\n",
    "def count_alignment(row_a, row_b, pos):\n",
    "  # =======|MATCH_BITS]        row_a\n",
    "  #    [===|MATCH_BITS|======  row_b\n",
    "  #        ^\n",
    "  #        pos\n",
    "  #    ~~~~~~~~~~~~~~~~ overlap\n",
    "  overlap = pos + MATCH_BITS\n",
    "  count = np.sum(row_a[-overlap:] == row_b[:overlap])\n",
    "  return (count, overlap)\n",
    "\n",
    "# Take the candidate successors and only keep the\n",
    "# ones that actually exactly match in their overlaps\n",
    "def prune_successors(candidates, rowidx, rows):\n",
    "  successors = []\n",
    "  for (candidx, pos) in candidates:\n",
    "    (count, total) = count_alignment(rows[rowidx, :], rows[candidx, :], pos)\n",
    "    if count == total:\n",
    "      successors.append((candidx, pos, count))\n",
    "  return successors\n",
    "\n",
    "def create_successors(noise):\n",
    "  succ = {}\n",
    "  cand = create_possible_successors_map(noise)\n",
    "  tails = calculate_row_tails(noise)\n",
    "  for (rowidx, tail) in enumerate(tails):\n",
    "    succ[rowidx] = prune_successors(cand[tail], rowidx, noise)\n",
    "  return succ\n",
    "\n",
    "succ = create_successors(binary_noise_stacked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we search that graph to find the longest chain of overlapping rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do this with memoization/dynamic programming:\n",
    "# we keep a dictionary ('longests') that keeps track of\n",
    "# the longest chain rooted at a given row.\n",
    "def _longest_rooted_chain(root, succ, longests):\n",
    "  if root in longests:\n",
    "    return longests[root]\n",
    "  # break out with special sentinel value if we hit a cycle\n",
    "  longests[root] = ((0, 0), -1)\n",
    "  max_length = ROWSIZE\n",
    "  max_succ = (-1, -1)\n",
    "  for (idx, pos, count) in succ[root]:\n",
    "    (_, sublength) = _longest_rooted_chain(idx, succ, longests)\n",
    "    if sublength == -1: # cycle!\n",
    "      return (None, -1)\n",
    "    length = ROWSIZE + (sublength - (pos + MATCH_BITS))\n",
    "    if length > max_length:\n",
    "      max_length = length\n",
    "      max_succ = (idx, pos)\n",
    "  (idx, pos) = max_succ\n",
    "  longests[root] = ((idx, pos), max_length)\n",
    "  return longests[root]\n",
    "\n",
    "# Then to find the globally longest chain, we just\n",
    "# try every possible starting point.\n",
    "def longest_chain(succ):\n",
    "  longests = {}\n",
    "  max_chain = -1\n",
    "  max_idx = -1\n",
    "  for rowidx in succ.keys():\n",
    "    (_, length) = _longest_rooted_chain(rowidx, succ, longests)\n",
    "    if length == -1: # cycle!\n",
    "      return -1, []\n",
    "    if length > max_chain:\n",
    "      max_chain = length\n",
    "      max_idx = rowidx\n",
    "  chain = [(max_idx, 0)]\n",
    "  curidx = max_idx\n",
    "  offset = 0\n",
    "  while True:\n",
    "    ((nextidx, pos), _) = longests[curidx]\n",
    "    if nextidx < 0:\n",
    "      break\n",
    "    curidx = nextidx\n",
    "    offset += ROWSIZE - (pos + MATCH_BITS)\n",
    "    chain.append((curidx, offset))\n",
    "  return max_chain, chain\n",
    "\n",
    "def assemble_chain(noise, chain):\n",
    "  chain_length = max(offset + ROWSIZE for (_, offset) in chain)\n",
    "  res = np.zeros(chain_length).astype(noise.dtype)\n",
    "  for (rowidx, offset) in chain:\n",
    "    res[offset:offset+ROWSIZE] = noise[rowidx, :]\n",
    "  return res\n",
    "\n",
    "chainsize, chain = longest_chain(succ)\n",
    "print(\"Found a chain of size:\", chainsize)\n",
    "print(\"Chain:\", chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_noisetable = assemble_chain(binary_noise_stacked, chain)\n",
    "# if everything went correctly you should have gotten a noise table that's 4559 long,\n",
    "# which is conveniently 47*97 so we can display it as rectangular image:\n",
    "if chainsize == 4559:\n",
    "  plt.imshow(initial_noisetable.reshape((47, 97)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding The Row Offsets\n",
    "\n",
    "Now, we find each noise row's position in the noise table by just trying every position and picking\n",
    "the one with the maximum similarity. \n",
    "\n",
    "Rows do not always have a perfect match in the noise table: they can differ by a few pixels. But we'll\n",
    "find that on average each row will have a >99% match to some location in the noise table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_row_offsets(noisetable, noise, normalize=True):\n",
    "  offsets = []\n",
    "  match_ratios = []\n",
    "  # transform from [0, 1] -> [-1, 1]\n",
    "  # (this is nicer for np.correlate)\n",
    "  if normalize:\n",
    "    table_normalized = noisetable.astype(float) * 2.0 - 1.0\n",
    "    noise_normalized = noise.astype(float) * 2.0 - 1.0\n",
    "  else:\n",
    "    table_normalized = noisetable\n",
    "    noise_normalized = noise\n",
    "  rowsize = noise.shape[1]\n",
    "  for (rowidx, row) in enumerate(noise):\n",
    "    # we use np.correlate to find the position of maximum correlation\n",
    "    # (this is vastly faster than for-looping ourselves)\n",
    "    pos = np.argmax(np.correlate(\n",
    "        table_normalized, noise_normalized[rowidx, :]))\n",
    "    count = np.sum(row == noisetable[pos:pos+rowsize])\n",
    "    offsets.append(pos)\n",
    "    match_ratios.append(count / rowsize)\n",
    "  return offsets, match_ratios\n",
    "\n",
    "(row_offsets, match_ratios) = find_row_offsets(initial_noisetable, binary_noise_stacked)\n",
    "print(\"Mean match ratio:\", np.mean(match_ratios))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refining the Noise Table\n",
    "\n",
    "The noise table we have already is pretty good, but since rows can differ by a few pixels we can try to\n",
    "get a better noise table by using the offsets we just calculated to figure out the value in the noise\n",
    "table that is *most consistent* with the rows that overlap it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a noise image and each row's offset into a noisetable,\n",
    "# produce the noisetable that is *most consistent* with the image\n",
    "def maximally_consistent_noisetable(offsets, noise):\n",
    "  tablesize = np.max(offsets) + ROWSIZE\n",
    "\n",
    "  # Place each row into the noise table and keep track of\n",
    "  # how many 1s we see at each position, in addition to how many\n",
    "  # rows 'see' that position in the table at all\n",
    "  nones = np.zeros(tablesize)\n",
    "  ncounts = np.zeros(tablesize)\n",
    "  for row, offset in enumerate(offsets):\n",
    "    nones[offset:offset+468] += noise[row, :]\n",
    "    ncounts[offset:offset+468] += 1\n",
    "\n",
    "  # the output noise table\n",
    "  noisetable = np.zeros(tablesize)\n",
    "\n",
    "  # set each noise table value to be the most common value\n",
    "  # it has seen\n",
    "  for idx in range(tablesize):\n",
    "      none = nones[idx]\n",
    "      ncount = ncounts[idx]\n",
    "      if none > ncount / 2:\n",
    "          noisetable[idx] = 1\n",
    "\n",
    "  return noisetable\n",
    "\n",
    "refined_noisetable = maximally_consistent_noisetable(\n",
    "    row_offsets, binary_noise_stacked)\n",
    "\n",
    "(refined_row_offsets, match_ratios) = find_row_offsets(\n",
    "    refined_noisetable, binary_noise_stacked)\n",
    "print(\"New mean match ratio:\", np.mean(match_ratios))\n",
    "\n",
    "if len(initial_noisetable) == 4559:\n",
    "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "  ax1.set_title('Initial Noisetable')\n",
    "  ax1.imshow(initial_noisetable.reshape((47, 97)))\n",
    "  ax2.set_title('Refined Noisetable')\n",
    "  ax2.imshow(refined_noisetable.reshape((47, 97)))\n",
    "  ax3.set_title('Difference')\n",
    "  diff = initial_noisetable.astype(float) - refined_noisetable.astype(float)\n",
    "  ax3.imshow(diff.reshape((47, 97)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that refining the noise table changed only a few dozen pixels and very slightly improved the match ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructing the Noise\n",
    "\n",
    "Now, we can reconstruct the binary noise, and see how well it matches the binary noise we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(noisetable, offsets):\n",
    "  return np.vstack([noisetable[o:o+ROWSIZE] for o in offsets])\n",
    "\n",
    "reconstructed_noise = reconstruct(refined_noisetable, refined_row_offsets)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "ax1.set_title('Original Noise (Green Channel)')\n",
    "ax1.imshow(binary_noise_stacked[IMAGEHEIGHT:IMAGEHEIGHT*2,:])\n",
    "ax2.set_title('Reconstructed Noise (Green Channel)')\n",
    "ax2.imshow(reconstructed_noise[IMAGEHEIGHT:IMAGEHEIGHT*2,:])\n",
    "ax3.set_title('Difference')\n",
    "noise_diff = binary_noise_stacked.astype(float) - reconstructed_noise.astype(float)\n",
    "ax3.imshow(np.abs(noise_diff[IMAGEHEIGHT:IMAGEHEIGHT*2,:]))\n",
    "\n",
    "diff_pixels = np.sum(np.abs(noise_diff))\n",
    "print(f\"Total different pixels: {diff_pixels} / {noise_diff.size} ({100 * diff_pixels / noise_diff.size}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering the Noise Magnitude\n",
    "\n",
    "Now we need to recover the magnitude values of the noisetable in addition to the signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accum_magnitudes(half_magnitudes, offsets):\n",
    "  maglists = collections.defaultdict(list)\n",
    "  for rowidx, offset in enumerate(offsets):\n",
    "    row = half_magnitudes[rowidx, :]\n",
    "    for colidx in range(len(row)):\n",
    "      val = row[colidx]\n",
    "      if val > 0:\n",
    "        maglists[offset + colidx].append(val)\n",
    "  return maglists\n",
    "\n",
    "def visualize_maglists(maglists):\n",
    "  most_mags = max(len(v) for v in maglists.values())\n",
    "  max_idx = max(maglists.keys())\n",
    "  ret = np.zeros((most_mags, max_idx+1))\n",
    "  for idx, mags in maglists.items():\n",
    "    sorted_mags = sorted(mags)\n",
    "    ret[:len(sorted_mags), idx] = np.array(sorted_mags)\n",
    "  return ret\n",
    "\n",
    "def collapse_mags(maglists, outsize):\n",
    "  max_idx = max(maglists.keys())\n",
    "  finalmags = np.zeros((outsize))\n",
    "  ambigs = np.zeros((outsize))\n",
    "  allzero = np.ones((outsize))\n",
    "  for idx, mags in maglists.items():\n",
    "    amags = np.array(mags)\n",
    "    if len(amags) > 0:\n",
    "      allzero[idx] = 0\n",
    "      modals, counts = np.unique(amags, return_counts=True)\n",
    "      modal = modals[np.argmax(counts)]\n",
    "      minval = np.min(amags)\n",
    "      finalmags[idx] = np.mean(amags) #modal\n",
    "      if minval != modal: #np.max(amags):\n",
    "        ambigs[idx] = 1\n",
    "  return (finalmags, ambigs, allzero)\n",
    "\n",
    "maglists = accum_magnitudes(magnitude_noise_stacked, refined_row_offsets)\n",
    "(noise_magnitudes, ambig_magnitudes, allzero_magnitudes) = collapse_mags(maglists, len(refined_noisetable))\n",
    "\n",
    "if len(noise_magnitudes) == 4559:\n",
    "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "  ax1.set_title('Noise Magnitudes')\n",
    "  ax1.imshow(noise_magnitudes.reshape((47, 97)))\n",
    "  ax2.set_title('Ambiguous Magnitudes')\n",
    "  ax2.imshow(ambig_magnitudes.reshape((47, 97)))\n",
    "  ax3.set_title('Magnitudes With no Data')\n",
    "  ax3.imshow(allzero_magnitudes.reshape((47, 97)))\n",
    "\n",
    "#magvis = visualize_maglists(maglists)\n",
    "#omniwrite(\"magvis.png\", magvis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(noise_magnitudes, bins=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_noise = (refined_noisetable * 2 - 1) * noise_magnitudes\n",
    "plt.imshow(signed_noise.reshape((47, 97)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(signed_noise, bins=128)\n",
    "plt.title(\"Histogram of Signed Noise Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magdeltas = []\n",
    "for _, maglist in maglists.items():\n",
    "  if len(maglist) < 10:\n",
    "    continue\n",
    "  ml = np.array(maglist) #.astype(float)\n",
    "  modals, counts = np.unique(ml, return_counts=True)\n",
    "  modal = modals[np.argmax(counts)]\n",
    "  minval = np.min(ml)\n",
    "  magdeltas.append(minval)\n",
    "#magdeltas = np.concatenate(magdeltas)\n",
    "_ = plt.hist(magdeltas, bins=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magdeltas = []\n",
    "for _, maglist in maglists.items():\n",
    "  if len(maglist) == 0:\n",
    "    continue\n",
    "  ml = np.array(maglist) #.astype(float)\n",
    "  modals, counts = np.unique(ml, return_counts=True)\n",
    "  modal = modals[np.argmax(counts)]\n",
    "  minval = np.min(ml)\n",
    "  magdeltas.append(modal - minval)\n",
    "#magdeltas = np.concatenate(magdeltas)\n",
    "_ = plt.hist(magdeltas, bins=min(256, len(np.unique(magdeltas))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repro_signed_noise = reconstruct(signed_noise, refined_row_offsets)\n",
    "qrstack = np.vstack([qrcode_clean*255]*3)\n",
    "qrdirtied = qrstack.astype(float) + repro_signed_noise\n",
    "qrdirtied = np.maximum(0.0, np.minimum(255, qrdirtied))\n",
    "qrdirtied = qrdirtied #.astype(np.uint8)\n",
    "qrrepro = qrdirtied #destack(qrdirtied)\n",
    "reprodiff = qrrepro - stack(qrcode).astype(float)\n",
    "cv2.imwrite(\"reproduced_qr_minval_noise.png\", destack(qrrepro).astype(np.uint8))\n",
    "plt.imshow(reprodiff.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rmse error:\", np.mean(reprodiff ** 2.0) ** 0.5)\n",
    "print(\"mean abs diff:\", np.mean(np.abs(reprodiff)))\n",
    "reprovized = np.abs(reprodiff * 30.0)\n",
    "omniwrite(\"hmmmmmmm.png\", reprovized)\n",
    "cv2.imwrite(\"magnified_reproduction_error.png\", destack(reprovized).astype(np.uint8))\n",
    "_ = plt.hist(reprodiff.reshape((-1)), bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniwrite(\"isolated_noise_stacked.png\", stack(isolated_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_mags = np.abs(repro_signed_noise)\n",
    "omniwrite(\"reproduced_unsigned_mags_minval.png\", just_mags)\n",
    "masked = just_mags * (stack(isolated_noise) > 0)\n",
    "just_mag_diff = np.abs(masked - stack(isolated_noise).astype(float))\n",
    "omniwrite(\"reproduced_mag_diff_minval.png\", just_mag_diff)\n",
    "print(\"mean abs diff:\", np.mean(just_mag_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_diff = masked - stack(isolated_noise).astype(float)\n",
    "qrcode_clean_stack = np.vstack([qrcode_clean]*3)\n",
    "modified_signed_diff = signed_diff * (qrcode_clean_stack * 2.0 - 1.0)\n",
    "\n",
    "print(np.mean(signed_diff))\n",
    "plt.imshow(signed_diff.T < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.correlate(modified_signed_diff[210,:], signed_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(secondary_offsets, _) = find_row_offsets(signed_noise, signed_diff, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(secondary_offsets[0:10])\n",
    "print(refined_row_offsets[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cursory Observations of the Binary Noise Table\n",
    "\n",
    "So the 4559 binary noise table itself looks very similar to uniform random binary data. The number of 0s and 1s is very balanced, so no surprises there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(refined_noisetable, bins=2)\n",
    "plt.title(\"Histogram of Binary Noise Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, qualitatively, the noisetable also looks very much like uniform random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(refined_noisetable) == 4559:\n",
    "  genuine_random_noisetable = np.random.randint(0, 2, refined_noisetable.shape)\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "  fig.suptitle('Visual Comparison of Extracted Noisetable with Uniform Random Data')\n",
    "  ax1.set_title('Extracted (Real) Noisetable')\n",
    "  ax1.imshow(refined_noisetable.reshape((47, 97)))\n",
    "  ax2.set_title('Uniform Random Noisetable')\n",
    "  ax2.imshow(genuine_random_noisetable.reshape((47, 97)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Initial Analysis of the Offsets\n",
    "\n",
    "### Histograms and plots why not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "fig.suptitle('Plotting the Offsets')\n",
    "ax1.set_title('Histogram of Offsets')\n",
    "ax1.hist(refined_row_offsets, bins=100)\n",
    "ax2.set_title('Plot of Offsets (Green Channel)')\n",
    "ax2.plot(refined_row_offsets[468:468+468])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing obviously stands out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Finding Repeated Sequences\n",
    "\n",
    "Since we know every row's location in the noise table, we can now semi-efficiently look\n",
    "for repeated sequences of noise (because they will consist of a repeated sequence of row offsets).\n",
    "\n",
    "For this part, we are going to find every repeat that is at least two rows large: note that since\n",
    "there are 4559 - 468 = 4091 possible offsets, the expectation of seeing a repeat of length two\n",
    "by chance is roughly on the order of (468*3)/(4091^2) ~= one in ten thousand. So if the offsets were\n",
    "generated uniformly at random, we really wouldn't expect to see even a single small repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identical_run_length(seq, p0, p1):\n",
    "  count = 0\n",
    "  for a, b in zip(seq[p0:], seq[p1:]):\n",
    "    if a != b:\n",
    "      break\n",
    "    count += 1\n",
    "  return count\n",
    "\n",
    "def find_repeated_offset_sequences(offsets):\n",
    "  # first, build a map from offset -> row[]\n",
    "  offset_positions = collections.defaultdict(list)\n",
    "  for rowidx, offset in enumerate(offsets):\n",
    "    offset_positions[offset].append(rowidx)\n",
    "\n",
    "  runs = collections.defaultdict(int)\n",
    "  runmatches = collections.defaultdict(list)\n",
    "\n",
    "  for offset, positions in offset_positions.items():\n",
    "    if len(positions) <= 1:\n",
    "      # this offset only occurs once so can't repeat\n",
    "      continue\n",
    "    for row_a, row_b in itertools.combinations(positions, 2):\n",
    "      runlen = identical_run_length(offsets, row_a, row_b)\n",
    "      if runlen > 1:\n",
    "        earlier_row = min(row_a, row_b)\n",
    "        runs[earlier_row] = runlen\n",
    "        runmatches[earlier_row].append(max(row_a, row_b))\n",
    "\n",
    "  run_positions = sorted(runs.keys())\n",
    "  for rowidx in run_positions:\n",
    "    rl = runs[rowidx]\n",
    "    if rl <= 1:\n",
    "      continue\n",
    "    for idx in range(1, rl+1):\n",
    "      if runs[rowidx+idx] == rl-idx:\n",
    "        runs[rowidx+idx] = 0\n",
    "\n",
    "  ret = []\n",
    "  for rowidx, run_length in runs.items():\n",
    "    if run_length > 1:\n",
    "      ret.append((run_length, rowidx, runmatches[rowidx][0]))\n",
    "  return ret\n",
    "\n",
    "def rowpos_to_imagepos(rowpos, channelnames=['B', 'G', 'R']):\n",
    "  channel = channelnames[rowpos // IMAGEHEIGHT]\n",
    "  ypos = rowpos % IMAGEHEIGHT\n",
    "  return (channel, ypos)\n",
    "\n",
    "def print_repeats(repeats, channelnames=['B', 'G', 'R']):\n",
    "  for (repeat_length, row_a, row_b) in repeats:\n",
    "    (ca, ya) = rowpos_to_imagepos(row_a, channelnames)\n",
    "    (cb, yb) = rowpos_to_imagepos(row_b, channelnames)\n",
    "    print(f\"Repeat length {repeat_length} @ {ca}{ya}-{ya+repeat_length} matching {cb}{yb}-{yb+repeat_length}\")\n",
    "\n",
    "repeats = find_repeated_offset_sequences(refined_row_offsets)\n",
    "print_repeats(repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, unlike the noisetable itself which doesn't have any obvious non-random characteristics, the\n",
    "offsets into that noisetable have six repeats, five of which are so substantially large as to \n",
    "be effectively impossible by chance.\n",
    "\n",
    "Visualizing the locations of the repeats as just solid colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_repeats(reps):\n",
    "  # put more colors here if you need them\n",
    "  COLORS = [100, 150, 200, 240, 50, 60, 70, 80] \n",
    "\n",
    "  dest = np.zeros((IMAGEHEIGHT*3, ROWSIZE), dtype=np.uint8)\n",
    "  for color, (length, a, b) in zip(COLORS, reps):\n",
    "    dest[a:a+length] = color\n",
    "    dest[b:b+length] = color+10\n",
    "\n",
    "  return dest\n",
    "\n",
    "# This function isn't used at the moment\n",
    "def crop_to_repeats(noise, offsets, repeats):\n",
    "  out_img_parts = []\n",
    "  out_offsets = []\n",
    "  for (length, a, b) in repeats:\n",
    "    out_offsets.extend(offsets[a:a+length])\n",
    "    out_img_parts.append(noise[a:a+length,:])\n",
    "  return (out_offsets, np.vstack(out_img_parts))\n",
    "\n",
    "repeats_img = vis_repeats(repeats)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Locations of Repeated Rows of Noise')\n",
    "ax1.set_title('Blue Channel')\n",
    "ax1.imshow(repeats_img[:IMAGEHEIGHT,:])\n",
    "ax2.set_title('Green Channel')\n",
    "ax2.imshow(repeats_img[IMAGEHEIGHT:2*IMAGEHEIGHT,:])\n",
    "ax3.set_title('Red Channel')\n",
    "ax3.imshow(repeats_img[2*IMAGEHEIGHT:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for not merely identical sequences (like 10, 30, 20 matching 10, 30, 20 elsewhere) but also \n",
    "sequences with additive shifts (e.g., 10, 30, 20 matching 15, 35, 25) by instead of directly searching the offsets,\n",
    "searching the deltas or derivatives of the offsets (so 10, 30, 20 becomes 0, 20, -10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_arr = np.array(refined_row_offsets)\n",
    "offset_deltas = offset_arr[1:] - offset_arr[:-1]\n",
    "repeat_deltas = find_repeated_offset_sequences(offset_deltas)\n",
    "print_repeats(repeat_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finds only the repeats we already knew about (except the length 2 repeat because taking the delta reduces lengths by one). \n",
    "\n",
    "Given the strange mirror symmetry, what about mirrored (reversed) sequences? To search for that we can concatenate the offsets with their\n",
    "reverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "offset_mirrored = np.hstack([offset_arr, np.flip(offset_arr)])\n",
    "offset_mirrored_delta = offset_mirrored[1:] - offset_mirrored[:-1]\n",
    "mirror_chans = ['B', 'G', 'R', '-R', '-G', '-B']\n",
    "\n",
    "print(\"== Mirrored ==\")\n",
    "print_repeats(find_repeated_offset_sequences(offset_mirrored), mirror_chans)\n",
    "\n",
    "print(\"== Mirrored Deltas ==\")\n",
    "print_repeats(find_repeated_offset_sequences(offset_mirrored_delta), mirror_chans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again this just finds the same sequences we already knew about.\n",
    "\n",
    "Out of the repeated sequences, the length 2 repeat is perhaps the most curious if only for its minimal size. Lets examine it in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hmm_offsets, hmm_noise) = crop_to_repeats(binary_noise_stacked, refined_row_offsets, repeats)\n",
    "print(\"min/max offset in repeat sections:\", np.min(hmm_offsets), np.max(hmm_offsets))\n",
    "coverage_map = np.zeros(refined_noisetable.shape)\n",
    "for offset in hmm_offsets:\n",
    "  coverage_map[offset:offset+1] += 1\n",
    "  coverage_map[offset+ROWSIZE-1:offset+ROWSIZE] += 1\n",
    "cmap = coverage_map.reshape((47,97))\n",
    "plt.imshow(cmap)\n",
    "#print(np.sum(coverage_map.reshape((47, 97)), axis=1))\n",
    "omniwrite(\"cmap.png\", 255.0*(cmap / np.max(cmap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(offset_arr[ROWSIZE*2+117:ROWSIZE*2+119])\n",
    "print(offset_arr[ROWSIZE*2+466:ROWSIZE*2+ROWSIZE])\n",
    "print(3317 + 1358)\n",
    "print(3317 - 1358)\n",
    "\n",
    "def stretch(data, n=32):\n",
    "  return np.repeat(data.reshape((1, -1)), n, axis=0)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(18, 5))\n",
    "a = refined_noisetable[1358:1358+ROWSIZE]\n",
    "b = refined_noisetable[3317:3317+ROWSIZE]\n",
    "ax1.imshow(stretch(a))\n",
    "ax2.imshow(stretch(b))\n",
    "ax3.imshow(stretch(a.astype(np.uint8) ^ b.astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat length 66 @ R168-234 matching R234-300\n",
    "r66_offsets = offset_arr[ROWSIZE*2+168:ROWSIZE*2+234]\n",
    "r66_deltas = r66_offsets[1:] - r66_offsets[:-1]\n",
    "print(r66_deltas)\n",
    "M = 97\n",
    "print(r66_offsets[:33] % M)\n",
    "print(r66_offsets[33:] % M)\n",
    "print((r66_offsets[33:] - r66_offsets[:33]) % M)\n",
    "print((r66_offsets[33:] - np.flip(r66_offsets[:33])) % M)\n",
    "\n",
    "def tobits(v, n=12):\n",
    "  return [(v//(2**i))%2 for i in range(n)]\n",
    "\n",
    "print(np.min(r66_offsets), np.max(r66_offsets))\n",
    "\n",
    "print(tobits(3317))\n",
    "print(tobits(1358))\n",
    "print(np.array(tobits(3317)) ^ np.array(tobits(1358)))\n",
    "\n",
    "#plt.scatter(r66_offsets[:33], np.flip(r66_offsets[33:]))\n",
    "\n",
    "# tempo = (r66_offsets[33:] - r66_offsets[:33])\n",
    "\n",
    "# tempimages = []\n",
    "# for offsetoffset in range(4096 - 4041):\n",
    "#   tempimages.append(np.vstack([np.array(tobits(o)) for o in (tempo + offsetoffset) % 1024]))\n",
    "# plt.imshow(np.hstack(tempimages))\n",
    "# omniwrite(\"offset_binary_shifts.png\", np.hstack(tempimages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_repeats(find_repeated_offset_sequences(secondary_offsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "* There are no repeats in the blue channel.\n",
    "* The red and green channels each have three repeats.\n",
    "* The repeats in both channels are arranged in an ABCCAB pattern.\n",
    "* Every repeat has either its start or end aligned to an exact quarter of the image height (multiple of 117).\n",
    "* Every repeat has an even length (6 repeats, so about a 1/64th chance).\n",
    "\n",
    "The number and size of the repeats rule out coincidence, while the arrangement of the repeats \n",
    "seems to bear the hallmarks of deliberate placement for some purpose.\n",
    "\n",
    "The alignment of the repeats has a kind of mirror symmetry around the vertical center of the image,\n",
    "but it's not clear why. Does this suggest that some kind of folding operation needs to be done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Against The Malfunctioning PRNG Hypothesis\n",
    "\n",
    "Initially I suspected that the curious structure of the noise could be due to poor\n",
    "PRNG choice or usage. For example, LCGs (linear congruential generators) are notorious for producing\n",
    "low-period output in their least significant bits, so that a careless programmer who reduces the range\n",
    "of a random number by modulo (e.g., `rand() % 2` to produce a binary value) can end up with random values\n",
    "with very short periods.\n",
    "\n",
    "At this point we can see some immediate problems in this hypothesis:\n",
    "\n",
    "* The noise table has a strange size (4559) and more importantly isn't a cycle\n",
    "* A bad LCG would produce low-period output everywhere rather than irregularly sized repeated patches\n",
    "* There are no repeated sections in the blue channel: a bad PRNG shouldn't be selective like this\n",
    "* A bad PRNG wouldn't align itself to 4ths of the image unless its period happened to be \"just so\"\n",
    "\n",
    "Nevertheless, it would be really convenient if we could declare this mystery solved as the result of\n",
    "bad programmers picking a perversely bad PRNG, so let's not let mere arguments stop us. Instead, let\n",
    "us pick a perversely bad PRNG and see what the machinery we have build does to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the LCG that Noita likes to use (MINSTD, a version of\n",
    "# https://en.wikipedia.org/wiki/Lehmer_random_number_generator )\n",
    "\n",
    "LCG_A = 7**5\n",
    "LCG_M = (2**31 - 1)\n",
    "def noita_lcg_next(v):\n",
    "  return (LCG_A * v) % LCG_M\n",
    "\n",
    "LCG_SEED = 0xd00df00d\n",
    "\n",
    "# generate an image from a prng next function and a seed\n",
    "def prng_image(w, h, prng_next, seed):\n",
    "  res = np.zeros((w*h), dtype=np.int64)\n",
    "  prng_state = seed\n",
    "  for pos in range(w*h):\n",
    "    prng_state = prng_next(prng_state)\n",
    "    res[pos] = prng_state\n",
    "  return res.reshape((h,w))\n",
    "\n",
    "noita_lcg_noise_raw = prng_image(468, 468*3, noita_lcg_next, LCG_SEED)\n",
    "\n",
    "# do the bad thing and just %2 it to get binary noise\n",
    "noita_lcg_noise_mod = noita_lcg_noise_raw % 2\n",
    "\n",
    "# do a slightly less bad thing and convert it to floats\n",
    "noita_lcg_noise_float = noita_lcg_noise_raw.astype(np.float64) / LCG_M\n",
    "noita_lcg_binary_float = (noita_lcg_noise_float < 0.5).astype(float)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle(\"Noise produced by Noita's LCG (minstd)\")\n",
    "ax1.set_title('Mod 2')\n",
    "ax1.imshow(noita_lcg_noise_mod[:100,:100])\n",
    "ax2.set_title('Float scaled')\n",
    "ax2.imshow(noita_lcg_binary_float[:100,:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now see if we can extract a noise chain of any sort\n",
    "def test_noise(title, noise):\n",
    "  print(title)\n",
    "  succ = create_successors(noise)\n",
    "  chainsize, chain = longest_chain(succ)\n",
    "  if chainsize < 0:\n",
    "    print(\"Cycle detected, but the logic to extract a cycle hasn't been written yet!\")\n",
    "    return\n",
    "  print(\"Longest chain: \", chain)\n",
    "  print(\"Chain length: \", chainsize)\n",
    "  return chain\n",
    "\n",
    "print(\"Note that it will always give you a chain of a single row (468)!\\n\")\n",
    "\n",
    "# Run the real noise again just to convince ourselves our test actually works!\n",
    "_ = test_noise(\"The actual QR code noise:\", binary_noise_stacked)\n",
    "print(\"\")\n",
    "_ = test_noise(\"Mod 2 MINSTD Noise:\", noita_lcg_noise_mod)\n",
    "print(\"\")\n",
    "_ = test_noise(\"Float-scaled MINSTD noise:\", noita_lcg_binary_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LCG that Noita actually uses in game did not result in any noise chains or cycles (note that the algorithm will always tell you\n",
    "that there's a chain that's a single-row long, because every row can be considered a trivial chain by itself).\n",
    "\n",
    "But, but... what if we make an even worse LCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_LCG_A = 83\n",
    "BAD_LCG_M = (2**21 - 1) # try changing the exponent here\n",
    "def incredibly_bad_lcg_next(v):\n",
    "  return (BAD_LCG_A * v) % BAD_LCG_M\n",
    "\n",
    "terrible_lcg_noise = prng_image(468, 468*3, incredibly_bad_lcg_next, 3) % 2\n",
    "plt.imshow(terrible_lcg_noise[:100, :100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_noise(\"Terrible noise:\", terrible_lcg_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Yes, you can make a really bad LCG that repeats. But the repeats are both very regular (because \"periodic\" means \"periodic\") and very visible.\n",
    "\n",
    "It's hard to prove a negative: _could_ there be some PRNG that exhibits the characteristics of the QRCode noise? I can't rule it\n",
    "out. But the problem is that:\n",
    "\n",
    "* Noita already uses MINSTD\n",
    "* MINSTD can be implemented in one line\n",
    "* MINSTD is already considered a bad PRNG\n",
    "* Every common PRNG in use these days is better than MINSTD\n",
    "* Yet even MINSTD doesn't create noise that can be compressed into a tiny noise table!\n",
    "\n",
    "So I think the idea that the Nolla programmers went out of their way, just for the QR code, to engineer\n",
    "a special PRNG that's stranger and worse than the MINSTD they already use, and instead of using any of the\n",
    "better PRNGs common available, is quite implausible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Creating Our Own Noise\n",
    "\n",
    "A reasonable question to ask is, 'how complicated is it to make such noise'?\n",
    "If intentionally creating noise like in the qrcode is very difficult, then that\n",
    "might be evidence that it is an unintended artifact of some other process. So \n",
    "lets try to reproduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick how large we want our tables to be\n",
    "# We don't have to to 4559!\n",
    "SYNTH_TABLE_SIZE = 7777\n",
    "\n",
    "# Create a signed noisetable\n",
    "synth_noise_table = np.random.normal(0.0, 50.0, size=(SYNTH_TABLE_SIZE))\n",
    "\n",
    "# Create random offsets\n",
    "# (note: the actual qrcode offsets have \"weird stuff\" going on but we don't understand that)\n",
    "synth_offsets = np.random.randint(0, SYNTH_TABLE_SIZE-ROWSIZE, size=IMAGEHEIGHT*3)\n",
    "\n",
    "# Construct the base grayscale noise from gray table & offsets\n",
    "synth_noise = np.vstack([synth_noise_table[o:o+ROWSIZE] for o in synth_offsets])\n",
    "\n",
    "# Load in our 'secret' image and invert it into the noise\n",
    "secret_image = (cv2.imread(\"mobydick.png\")[:,:,0] > 0).astype(np.uint8)\n",
    "secret_image = np.vstack([secret_image]*3) # for three channels\n",
    "synth_noise[secret_image > 0] *= -1.0\n",
    "\n",
    "# stack back into color image\n",
    "synth_img = destack(synth_noise)\n",
    "\n",
    "# add in visible QRCode and clamp and convert to uint8\n",
    "for chan in range(3):\n",
    "  synth_img[:,:,chan] += (qrcode_clean*255.0)\n",
    "synth_img = np.maximum(0.0, np.minimum(255.0, synth_img))\n",
    "synth_img = synth_img.astype(np.uint8)\n",
    "\n",
    "plt.imshow(synth_img)\n",
    "cv2.imwrite(\"moby_dick_qr.png\", synth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try solving our own image\n",
    "def solve_image(img, signed_noise=True):\n",
    "  (_, resynth_bin) = full_preprocess(img, double_xor=signed_noise)\n",
    "  resynth_stacked = np.vstack(resynth_bin)\n",
    "  resynth_chain = test_noise(\"Our own noise:\", resynth_stacked)\n",
    "  initial_noisetable = assemble_chain(resynth_stacked, resynth_chain)\n",
    "  (row_offsets, match_ratios) = find_row_offsets(initial_noisetable, resynth_stacked)\n",
    "  print(\"Mean match ratio:\", np.mean(match_ratios))\n",
    "\n",
    "  refined_noisetable = maximally_consistent_noisetable(\n",
    "      row_offsets, resynth_stacked)\n",
    "\n",
    "  (refined_row_offsets, match_ratios) = find_row_offsets(\n",
    "      refined_noisetable, resynth_stacked)\n",
    "  print(\"New mean match ratio:\", np.mean(match_ratios))\n",
    "\n",
    "  reco = reconstruct(refined_noisetable, refined_row_offsets)\n",
    "  diff = reco - resynth_stacked\n",
    "  return (reco, diff)\n",
    "\n",
    "(reco_synth, diff_synth) = solve_image(synth_img, signed_noise=True)\n",
    "plt.imshow(np.abs(diff_synth[468*1:468*2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = np.abs(diff_synth)\n",
    "cv2.imwrite(\"moby_dick_extracted.png\", destack(dd.astype(np.uint8) * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
